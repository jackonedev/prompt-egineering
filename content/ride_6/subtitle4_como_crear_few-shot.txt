Let's look at some
examples of where we're making mistakes in our
few-shot examples. Things that we can do wrong in the formatting of
our prompts and what we can think about and
learn from these examples. Here's some few-shot examples. The idea behind this
is we put together a not very good prompt to
analyze how hard things are, like is it soft or hard? That's really what
we're trying to get at with this prompt and
some few-shot examples. Now this is not a good prompt. This is a bad prompt
and we'll see why. What we've got is we have input, as the prefix, we
then got brick. We've got output as a
prefix and then hard. We've got input as a prefix
and then pillow, output soft. Then we've got input car. Now, notice what we've got here. One, we don't have a lot
of examples here, but two, we've switched gears, we started talking about input and output
and the hardness of things using things that are
really obvious, probably. A brick is hard, a pillow is soft, I guess car is hard but it's not as clear what we're getting at. We also might just be
trying to describe some fundamental
intrinsic capability or properties of this thing. It may not be about hard or soft because it could
be other things. It could just be an attribute to describe the thing itself because pillows are
generally thought of as soft and bricks are
normally thought of as hard, those are fundamental
descriptors sometimes they use for them. What we see is that when we prompt ChatGPT in this example, what we get is, it says, fast or maybe transportation. This shows us a
couple of things. This is a prompt that's not very well-designed and
because of that, what we see is that ChatGPT
isn't quite sure what the next output should
be from the pattern. Now there's a couple
of mistakes in here. One is we've got
input and output. Input and output are
really generic prefixes. They don't really give
the language model much information about what the task is that
it's trying to do. All we know is we're taking in something and we're
getting out something, but it's not really telling us what it is that
we're describing. Maybe we should
go and say input, brick, surface feeling, hard, input, pillow, surface feeling, soft or maybe it's something
about if you hit it, what will it feel like? There's all kinds of things
we could do to give it more context around what
we're trying to get at. We can also give
it more examples. But what we've got
here overall in this prompt is we've got
a lack of information. It's not detailed
enough and we're not giving enough information
about what the task is that's being solved and
the result of that is that the language model isn't really sure what the solution is. Now one way we can solve this is we can go and before
we give our examples, we could give a little
bit more context, so we can provide
some instructions. We could say your output
can only be soft or hard. Now, one thing to note is as I'm putting these things in
quotation marks to try to indicate to the
language model that these are the two values, I don't want it to get
confused and think I'm talking about soft or hard in
some other contexts, but I'm putting these
things in quotation marks, giving them some emphasis that
either it's soft or hard. This may not always work
because we still have a prompt that's really generic. That really doesn't have a lot of context and
a lot of meat to it. It's still very vague. We've got input brick, we're still dealing with it. Now luckily in this case, adding this little bit
of additional context and specify to the language
model that we only want soft or hard as an
output helps it figure out that it should think about
if I pushed on this car, how would it feel like,
it would feel hard. That's the task that's being solved and so it comes
back with the right thing. But that's our first
thing to think about, when you're writing
few-shot examples, are your examples
detailed enough? Are your prefixes that you're using for these
things meaningful? Do they give additional
contexts that the large language
model can use to figure out what the next
thing in the sequence is. Another way to
think about it is, if you gave this to 10
different human beings, would you have a chance
of a few of them might be confused about what
you're trying to ask for? If the answer is yes, then you probably need to think about how to refine your prompt. If it's not fully clear
what you're trying to do in the examples of what the process behind the
examples is illustrating, then you're probably
not detailed enough. Now one way you can
solve that is by adding some more instruction like we have here at the
top of this prompt. Now let's look at
another example. I've got object
plane speed fast, object worm speeds slow and I go through a
series of examples. Notice I'm giving a little
bit more context here. My speed prefixes
a little better because speed seems something
that's quantifiable. Like a plane is fast. A worm is slow that
should be more contexts. We then go prompt it
with object ball speed and this is another
important thing to realize is it says variable. One of the things we have to
think about is we're trying to get the large
language model to produce an output or to classify or to fit
into some boxes. We need to make sure that it
gets enough information in the input to be able to accurately put things
into the boxes. We need to make sure that whatever information
we're giving it about what it's going to do, always has enough context that it can make a
decision about the label. Now in this case, a ball could
be fast, it can be slow. If I shoot a ball out of a candidate might be
going really fast. If I shoot a ball, if I put it on the
top of a rocket, it might be going really fast. There's all different
things that could make a ball go faster or slower. We have to make sure that
we provide enough context. Now, in general,
the thing to think about is am I given
enough information? Now I'm going to
go back. I'm going to change my prompt down. I'm going to say object
ball kicked by a baby. Then we get immediately
it says slow. When you're building up
the few-shot examples, so you want to make sure one, your prefixes are
pretty descriptive. Two, do the examples
that you're giving, really give enough information to derive what the
underlying processes that is going on in order to go from the starting input
to the final output. Do you really have enough information there to see what's underneath the hood? One way to think about that is if you asked a bunch of humans, would they all know exactly
what you wanted it to do without any ambiguity. Two, you want to make
sure that you have enough description
before the examples if needed to provide
additional context. Sometimes examples, you're just not going to be able to
get there with examples. You may need to provide
some additional rules or other information at the start of your prompt to help it along. Really a lot of times
what you're thinking about is you have a
limited amount of space in your prompt and
you have to think about is this
something that I can better describe and an example or is it something
that I can better describe in some
instructions at the top? What better means is will it get a more
accurate output and will it take less space or maybe less words
to describe it? If I can get it done very
clearly in an example, then maybe that's
the right way to go. That example doesn't
take a lot of space. On the other hand, if it's a really simple
rule and I can describe it, I may be able to get
a better response by putting that rule up
into the description. Then finally, we
have to make sure that we provide
enough information. In this example,
ball kicked by baby, if we're trying to ball kick things or we're trying to
get a particular output, we may have to make
sure that when we give input that there's
sufficient richness and detail in that input to follow the rules or a prior
examples that we've given. Now this is particularly
important if our prior examples
were pretty clear cut, is probably something better. Now, one could argue
that our prior examples, if we go back and look at this, we said a plane was fast. Well, a plane might not be fast, a plane might be slow. It might be sitting parked
on the tarmac of an airport. We also want to make
sure that our examples are rich enough and
detailed enough. Now sometimes we can get away with simple
things like plane, fast, worm, slow in
order to teach it. But sometimes we're going to
have to go into more detail. We're going to
provide more context. Make sure and think about, do you have enough
real information? Is it going to depend
on the situation, those things like
that provide details. Now one way to do it would be to say just make some assumptions. But one of the things that can happen sometimes when you do that is you may get
more verbose output, then you really want, it may
not fit into that bucket. But when you're
writing these prompts, think meaningfully about
what is a meaningful prefix? What information
or instructions do I need to have at
the top and can important information be better conveyed as an instruction
at the top or as an example? Do my examples cover the
space enough to have enough good examples
with rich detail for it to learn from and derive what
the underlying process is? Then when I go and I
give it the new input, do I make sure I have enough relevant contexts
and information with the new input in order to apply the process
that I taught? Because we may teach it
the process really well, but then given a new input
that just fundamentally is missing information in
order to apply that process.