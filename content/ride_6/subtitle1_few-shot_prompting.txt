A really interesting
thing that we can do in a prompt is we can
actually teach the large language model
to follow a pattern using something called few-shot
examples or few-shot prompting. The idea behind this is, we're going to give the large language model
examples of input that we would give it and then the output
that we would expect. You can imagine a human will go through and go through
a couple of examples. They'll give it some input, they'll show it what the correct output should look like, they'll then give
it another input, give it the correct output. The idea behind this is
that we're developing a pattern that the
large language model can then learn to follow. Actually within our prompt, we're teaching the large
language model a new trick. We're basically showing it what we want it to do
and how we want it to format the output within
a series of examples. Rather than describing the
process we want it to follow, we're giving it examples of
what we want it to do and then hoping that it will continue the pattern
that we provide. Let's take a look at this. If you'll see right
here, we've got a simple example of
few-shot prompting. The idea behind this is
we start off with input. This is a common
thing that you will see in few-shot prompting. Is that you'll see a prefix
applied to the lines. You can have all kinds of
prefixes and your prompts and your patterns don't have to
just be input and an output. It could be a couple
of different steps, and we'll look at
that in a minute. But as we see here, we have an input which is
the movie was a bit long. What we're going to do
is we're going to do what's called
sentiment analysis. This just basically
is try to analyze some comment about
a thing and try to figure out what the user
sentiment was about it. Were they positive about it? Did they think it
was a good thing? Were they negative about it? Did they think it was a bad
thing or were they neutral? We're just going to do
a really simple example with sentiment analysis. We're going to take some fake
reviews of different things and then we're
going to decide if they were positive,
negative, or neutral. What we're doing,
rather than saying, go and analyze
these user comments and tell me if
they are positive, neutral, or negative, instead, what we're going to do is give a series of examples. We give the first example
which was the movie was good but a bit long and we say the
sentiment was neutral. It has some pluses and minuses. It was good, but
it was too long. Then we have an example
where we say input, I didn't really like this book, it lacked important details and didn't end up making sense. Then we say the
sentiment was negative. Then we have another input where we say, I love this book. It was really helpful in learning how to
improve my gut health. The sentiment is positive. Then what we do is
we're going to give it the thing that we actually want to
perform the task on, that we're going to hopefully get it to follow the pattern on. We'll say input, I wasn't sure what to think
of this new restaurant. The service was slow, but the dishes were pretty good. Then we queue it in to follow the pattern by saying
sentiment: Basically, tell us what you think
about this thing. My prefixes are a
little bit leading. They are giving you
information about what to do. But it could also learn
from these patterns if we just said input and output,
it can also do that. I'm giving it a little bit
of richer information. My goal then is to get the
large language model to go and analyze the input I
give it and decide what should be
attached to sentiment. It picks up on our pattern
and it says neutral. It sees it in this review. It says service was slow, but dishes were pretty good. It's seeing that
positive and negative and it's following my
pattern that I gave it. I didn't tell it this
is what neutral meant, but in the examples
that I had before, a balance of positive and negative ended
up being neutral. A couple of things to note. One, we didn't tell
it what to do, two, we didn't tell it exactly what the output should look like. What we see is that it's
constrained itself to the output and then basically the labels that we
gave it earlier. We had negative,
positive and neutral, and it's automatically
constrained itself and provided an
output based on that. It won't always
constraint itself, but this is a nice thing as in many cases this can help us get a more constrained or
bounded output out of it. If we went and we gave it
a new one, we can say, input, I really
hated this coffee, it was roasted too much
and tasted burned. Notice, I'm not going to give it the prefix for sentiment. So let's see what
happens when I do that. Then you notice it provides the prefix and the
output that I expected. The really important thing
to think about with this is, if you go back to
when we talked about large language models
and what they're doing, they learn to predict next word. They're learning patterns. When it sees input
in a statement, it's predicting that
the next word should be sentiment because that's what it saw in the examples
that we've provided it. Before when we ended
with sentiment, then what it did
is it knew that it needed to pick up
right after sentiment. If we go back to
this and we edit it, and we just add a line
here and we say sentiment, then it should pick up the
pattern right after sentiment. This is helpful to
know about when you're thinking about
few-shot examples. This is a simple example
of following a pattern, but you can see how this is
a powerful way of teaching, particularly if you have data that describes the task
that you want to do. When I say describes, it shows the input and
it shows the output. It doesn't necessarily describe how the task was performed, but we can see
what's being done. That type of thing can be a good fit for a
few-shot prompt.