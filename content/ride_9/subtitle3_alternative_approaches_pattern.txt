Brainstorming different
approaches to solve a problem is a really interesting way of using a large language model and one of my
favorite uses for it. Something that I
really like about large language models is
that I can come up with a problem that I want to solve
and I can rapidly come up with different ways of
solving that problem and I can test them
out and get feedback. Now, one way to do that is using the alternative
approaches pattern. The idea behind this is
if we're going to have the large language model
within some particular scope, always suggest alternative
approaches for solving a particular problem or performing a particular task. That is, we always want
it to give us good ideas. Now, I say good ideas,
they may not be good, they may not even work, but like brainstorming
in a group. You may have a lot
of different ideas. Some of them may be great, some of them may be terrible, but just the fact
that everybody's thinking through ideas, throwing them up
on a Whiteboard or writing them down or
talking about them can help you do a better
job of thinking through the problem and
figuring out what is the right solution
that makes sense. I'm going to show you how to use the alternative
approaches pattern to essentially
perform brainstorming with the help of a
large language model. Now one of the things
to note is just like if you get a group
of participants that have different backgrounds, they may help you come
up with ideas that you yourself would not
have come up with. Well, a large language model was trained on a big corpus
of material and it's probably seen lots
of problems and seen discussions of
different solutions to that problem or different
alternative approaches for a particular problem. You can think about it. It
is in some ways learned information about how to solve problems in
different ways and can help us get access
to that knowledge. Now, it may also have never seen the exact problem
that we're asking it for and it may be able to synthesize using what
it seen in the past, interesting potential
approaches, even it hasn't seen the exact problem
that we're working with. Let's start off with
an example of this. We're going to do the
alternative approaches parameter, and we're
going to say from now on, if there are alternative ways to accomplish the same thing, list the best
alternative approaches. This is a very generic
wording for this, it's probably better
to structure it very precisely to the task
that you're working on, provide more rich context. But I'm going to show
you that even within a fairly generic wording, we can get this pattern
to work for us. Then I'm going to
say compare and contrast the alternatives and ask me which one I want to use. I'm adding a little
bit to this pattern. I'm saying given the
alternatives but also give me some information, compare and contrast
the alternatives. Maybe that'll help
me think through which one is going to better
fit my particular situation. I can see if what are
the pros and cons of each approach and which one is a better fit based on
those pros and cons. I then ask if you think back
to some of the past videos, we've looked at
using gameplay to create prompts and test us on our ability
to write prompts. One of the problems that ChatGPT generated for us is
it asked us to write a prompt to distinguish
leap years. Basically, we would
give it dates and the prompt should be able to determine if it's a
leap year or not. I'm going to turn that
into a task for ChatGPT. I'm asking ChatGPT to write several prompts for itself
to solve this task. I'm having ChatGPT write
the prompts for me. Now, I could have
introduced this earlier in the course and just said
any tasks that you have, you want to be better
at prompt engineering, ask ChatGPT to do the
prompt engineering for you. That's basically
what I'm doing here. Sometimes this
works, but I don't want you to think that this is the best approach that
I should just always go to ChatGPT and
ask it how to do it. Now, you should have
a sense yourself, just like any problem you're bringing into a large
language model, you have to own the content. You should have
some expertise in this area to help you distinguish what are
the better approaches, what things don't make sense, what's unsafe or
harmful in some way. You need to own whatever
content comes out. That's part of the reason
I didn't just start by saying prompt engineering, go ask ChatGPT for the prompt. We're not
going to do that. But in this case, we
are. We're saying, we want ChatGPT to
write a prompt to use few-shot examples to
determine if a date in the year, month, day format is a leap year and the
output should either be year is a leap year or
this is not a leap year. ChatGPT then says, approach
one direct question format. In this approach, we directly asked ChatGPT whether
a given data is a light leap year
and then it shows us the few-shot examples
for doing this. It's generating the prompt
force that we could use essentially to go and check if something
is a leap year. Then the second approach, it says conversational
format and then it uses a completely different style. It's still roughly
the same format of generating few-shot examples
to solve this problem. Now notice I scoped it
to few-shot examples. I could have left the scope more wide-open and just say
write a prompt to do this. But I thought it'd be fun
to look at different. Can we write the
few-shot examples in different styles and will
it make a difference and we could go and test
this and see if it does. We now see it generating, again, it's solving the
problem of showing few-shot examples
for leap years. Now, I noticed
something interesting. It's showing us how
to solve the problem. It didn't actually fully follow the format that I gave
it because I said, why is a leap year or
this is not a leap year in it is putting the date,
the year at the front. It's actually made a mistake and this is part of the reason why you should read your
prompts that it generates. Make sure they're doing
exactly what you want. But I could go and retell it
and fix it and refine it. But you notice now I've got
two different approaches to few-shot examples to
solve this problem. Then at the end, I get a comparison, it says approach one is
more formal and structured, making it easier to
understand and parse. It may be suitable for
applications where a clear and concise
answer is required. Then it says, approach to
his more conversational, natural making it more
engaging for users and maybe suitable for applications where user interaction
is important. Which is kind of true if
you're going to prompt the user and then have like, here's what the user said, here's what the assistant's
supposed to say. This would probably
make sense if you were just reporting on a Q&A, then maybe the other
format makes sense. It's a reasonable discussion of the different
alternative approaches for solving this problem. Now, let's look at
another example. I thought, what is a task that can
sometimes be burdensome? I thought about
email, now I haven't actually done this and
tried it out on my email. I still keep my e-mail personal, if I can to my laptop, but I thought that this would be an enhancement
in the future if I was running a large language
model directly on my laptop, something like Lama, Alpaca. Maybe I could do this and I
would be comfortable with it. I say whenever I
asked you to write a prompt for me to
accomplish a task, I'm going to change the alternative approaches
pattern slightly. I'm going to tweak it a
little bit for this one. What the task is, list alternatives for
completing the task, and then write a prompt
for yourself for each approach so
some basic format. Now I'm doing two things. One is I'm having it lists
the task at the beginning. Right here I'm saying list
the task and then I'm saying list alternative approaches for completing the test so this
is going to be upfront. One, it helps me know what it's approaches are going to be before
I even see them, I can see a preview, two this can actually be helpful for large
language models if you hadn't kind of
summarize the task and summarize different
approaches or reasoning. This is been shown to be
helpful in many cases. I'm actually asking it to do that at the beginning just to show you how you can
tweak the pattern and get some possible improvements. In this case, it does roughly the same thing
that we saw before. Then I asked me what I want
you to write a prompt for. The prompt I ask it is, I say write a prompt for ChatGPT to automatically
detect questions in a chain of emails and summarize everyone's questions as bullet points beneath each question. The idea behind this is really this key
part is at the end, you can imagine getting behind an e-mail and you want ChatGPT, or some other large
language model that you trust looking at your email, that you put it in the whole
email conversation chain, assuming that you are following appropriate privacy and other rules around whatever content is discussed in
that email chain. Maybe you start
with something like what movie or what restaurant people want
to go out to eat. When you have a
group discussion. You get to these long
discrete group discussions and they've gone off the rails. People have discussed other
things thrown in jokes, commented on other people's
jokes and by the end of it, you can't really decipher who wants to go to what
restaurant and why? Because there's all this other
random stuff there an end. You might go and say summarize the questions and
people's opinions on it and it'll give you maybe
just the discussion about the restaurant and
people's perspectives. You would dump
this end and I say write me a prompt to do that, and it gives me three
different approaches. One, write a prompt that
instructs ChatGPT to first identify all questions
in the email chain, and then extract responses
related to each question, and finally summarize the
opinions as bullet points. Approach 2, write a prompt
that asks ChatGPT to simulate a conversation where it plays
the role of a summarizer, and notice it's basically using the persona pattern here in
its own prompt for itself, asking the user to
provide the email chain, and then generating the
summary with bullet points, and then finally it says
write a prompt that presents the email
chain as a case study. That's an interesting approach, I wouldn't have thought of that. What's interesting
is a case study, it probably read lots
of case studies, so it's probably good
at thinking about how do you think through
a case study, and ask ChatGPT to
provide a summary report, that includes the questions and the corresponding opinions
in bullet points. Then it gives me each of
the different prompts. ChatGPT, your task is to analyze the following
chain of emails, and identify any questions
that were asked. For each question please
summarize the opinions expressed by the participants in the email chain as bullet
points beneath the question. It's generated its own
prompt for itself. Prompt Approach 2, ChatGPT, let's simulate a
conversation where you play the role of a summarizer and
it goes on to discuss it, and then it has a
slightly different format down here for how it
wants things presented, and then finally,
prompt for Approach 3, ChatGPT, consider
the following chain of emails as a case study. That's really interesting, and a completely
different approach. That what I just would not have thought of
from the beginning. Now, maybe if I went to
business school or something, I would be more thinking
of case studies, and this is a natural approach. But this is something
new to me and so I can learn from how
to write prompts, and now I have three completely different approaches
that I could go try out without having
to think of them myself. I've also learned something,
maybe case studies and presenting different points of view on an idea as a case study, is a good way to get
output from ChatGPT. Now, what would I
like to do next? Well, I'd like to
take my alternatives and I would like to
evaluate them some way, and one way to evaluate output, particularly when it's just text and you want to figure out
how good does it look? Does it seem to
solve the problem? Is just to use the
large language model to evaluate its own output, or to evaluate the output of another large language model. Now, usually you want to do this in a separate conversation. You don't necessarily
want to tell the large language model,
now evaluate yourself. Sometimes that leads
to worse behavior where it just says,
oh, I'm right. Sometimes what we want
to do is take that into a separate conversation
and evaluate it, and I'm basically asking it, how would you write
prompts to take your own output and evaluate? How would you write prompt
to evaluate the prompts? So I say write a
prompt to evaluate several different
prompts for summarizing the questions in an
email conversation, and the different stakeholders perspectives on the questions. Then it gives me three
alternative approaches again. Approach 1, write a prompt that presents multiple
prompts to ChatGPT, and ask it to evaluate each prompt's effectiveness in summarizing questions and
stakeholders perspectives. Approach 2, write a
prompt that instructs ChatGPT to simulate
a conversation where it plays the
role of an evaluator, and the user provides different
prompts for evaluation, again using the persona pattern, and then write a prompt
that asks ChatGPT to provide feedback on each prompt and suggest improvements if any. There's a different
take in each one. The last one is really about
how do we improve each one, second one is using the persona pattern
act as an evaluator, and the first one is really
to evaluate effectiveness. They're different takes
on the same idea, but now I have three completely
different ways that I could go and take my prompts
that it generated itself, and look at and evaluate
them using ChatGPT itself.